# Omni-Fedge
[![HitCount](http://hits.dwyl.com/ksanu1998/https://githubcom/ksanu1998/Omni-Fedge.svg)](http://hits.dwyl.com/ksanu1998/https://githubcom/ksanu1998/Omni-Fedge)<br>
<b>Federated Algorithm With Bayesian Approach: Omni-Fedge</b> <br>
<b>Authors:</b> Sai Anuroop Kesanapalli and B. N. Bharath <br>
Submitted to <b>ICASSP 2021</b> <br>
<hr>
<b>Abstract</b> <br>
In this paper, we consider the problem of Federated Learning (FL) under non-i.i.d data setting. We provide an improved estimate of the empirical loss at each node by using a weighted average of losses across nodes with a penalty term. These uneven weights to different nodes are assigned by taking a novel Bayesian approach to the problem where the problem of learning for each device/node is cast as maximizing the likelihood of a joint distribution. This joint distribution is for losses of nodes obtained by using data across devices for a given neural network of a node. We then provide a PAC learning guarantee on the objective function which reveals that the true average risk is no more than the proposed objective and the error term. We leverage this guarantee to propose an algorithm called Omni-Fedge. Using MNIST and Fashion MNIST data- sets, we show that the performance of the proposed algorithm is significantly better than existing algorithms. <br>
<b>Index Terms</b>— Federated Learning, Neural Network, Bayesian Approach, Distributed Machine Learning, PAC Learning.<br>
<hr>
Sai Anuroop Kesanapalli is a final year undergraduate in the Department of Computer Science and Engineering at IIT Dharwad. <br>
B. N. Bharath is an Assistant Professor in the Department of Electrical Engineering at IIT Dharwad. <br>
<hr><br>
© 2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
